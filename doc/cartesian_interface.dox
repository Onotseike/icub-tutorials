/**
@page icub_cartesian_interface The Cartesian Interface

\author Ugo Pattacini

\section sec_cart_contents Contents

- \ref sec_cart_intro
- \ref sec_cart_dependencies
- \ref sec_cart_opencloseinterface
- \ref sec_cart_coordsystem
- \ref sec_cart_useinterface
- \ref sec_cart_nonlineardesign
- \ref sec_cart_contextswitch
- \ref sec_cart_taskrefvel
- \ref sec_cart_eventscallbacks
- \ref sec_cart_siminterface


\section sec_cart_intro Introduction

The YARP <b>Cartesian Interface</b> has been conceived in order to enable the control of the arms
and the legs of the robot directly in the operational space, that is instead of commading new configuration
for the joints, user can require for instance the arm to reach a specified pose in the cartesian space, expressed
as a combination of a 3d point to be attained by the end-effector (the center of the palm in this case) along with
the desired orientation given in axis-angle representation.

Before proceeding, the reader might find helpful to recap some motor control fundamentals (click \ref icub_motor_control_tutorial "here")
as well as to be introduced to the underneath topic of forward and inverse kinematics by going through the \ref iKin documentation and tutorials.

The architecture of the Cartesian Interface is based on three components: 1) a complex nonlinear <b>optimizer</b> that handles the inverse kinematics problem;
2) a <b>controller</b> that represents the server part of the interface and is responsible for generating the human-like velocity profiles to move the system
from its initial state to the joints configuration found by the solver; 3) a <b>client</b> part that exposes a collection of C++ methods to abstract the interface
from its specific implementation. \n
Alternatively, the client can act as a pure wrapper over the solver coping with the inverse kinematics, whereas the user might consider developing his own controller
meeting some specific requirements. However, also in this configuration, the server part needs to be launched (even if not employed) as a gateway to let the client
communicate with the solver.

\note A paper that describes how the controller works can be downloaded <a href="http://eris.liralab.it/viki/images/c/cf/CartesianControllersEvaluation.pdf">here</a>.
<b>If you're going to use this controller for your work, please quote it within any resulting publication</b>: U. Pattacini, F. Nori, L. Natale, G. Metta, G. Sandini,
"An Experimental Evaluation of a Novel Minimum-Jerk Cartesian Controller for Humanoid Robots", <i>IEEE/RSJ International Conference on Intelligent Robots and Systems</i>,
Taipei, Taiwan, 2010.


\section sec_cart_dependencies Dependencies

In order to use the Cartesian Interface, make sure that the following steps are done: \n
[Note that the term <i>cluster</i> refers to the set of computers directly connected to the robot network, whereas <i>PC104</i>
indicates the hub board mounted on the robot]

-# Update YARP and iCub repositories.
-# Compile YARP (always a good practice).
-# Install <a href="http://eris.liralab.it/wiki/Installing_IPOPT">Ipopt</a> on the cluster.
-# On the cluster: compile the repository with the switch <b>ENABLE_icubmod_cartesiancontrollerclient</b> enabled. This will make
   the client part of the interface available.
-# On PC104: compile the repository with the switch <b>ENABLE_icubmod_cartesiancontrollerserver</b> enabled. This will make the
   server part of the interface available on the hub.
-# On PC104: create the files to configure the cartesian controllers.
   For example, to configure the cartesian arm controllers do:
   - copy the files <i>cartesianLeftArm.ini</i> and <i>cartesianRightArm.ini</i> from <i>$ICUB_ROOT/main/app/robots/iCubGenova01/conf</i>
     to <i>$ICUB_ROOT/main/app/robots/$ICUB_ROBOTNAME/conf</i>.
   - create/update the file <i>$ICUB_ROOT/main/app/robots/$ICUB_ROBOTNAME/conf/iCubInterface.ini</i> containing the following lines:
     \code
     config icub.ini    // or the current main configuration file
     cartLeftArm cartesianLeftArm.ini
     cartRightArm cartesianRightArm.ini
     \endcode


<b>iCubStartup Application Launcher</b> \n

Henceforth rely on the installed copy of <i>$ICUB_ROOT/main/app/iCubStartup/scripts/iCubStartup.xml.template</i> application
to launch both the iCubInterface and the cartesian solvers (and other useful tools as well). \n
The \ref iKinCartesianSolver "Cartesian Solvers" are required to invert the kinematics using Ipopt and they are meant to run
on the cluster to avoid overloading the PC104 core. The solvers need the iCubInterface to connect to the robot to get useful
data to operate (e.g. the joints bounds) and also iCubInterface needs them to open the server side of the Cartesian Interface.
Therefore the <i>chicken-egg</i> problem is solved by applying proper connection timeout to the solvers and to keep them as
close as possible to the iCubInterface by using the application.
Moreover, once everything is launched from within the application, if the user <i>accidentally</i> stops one of these modules,
it turns to be mandatory to restart all of them from the beginning; this comes from the requirement to keep the communication
protocol as light as possible, avoiding requests for integrity check and reinitialization. \n


\section sec_cart_opencloseinterface Opening and Closing the Cartesian Interface

The Cartesian Interface can be opened as a normal YARP interface resorting to the <i>PolyDriver</i> class but before
this the user has to enforce the dependency of his project from the iCub hardware modules collection.

To achieve that, the following lines have to be included in the <i>CMakeLists.txt</i> file whenever you intend to put
your project within the iCub repository:
\code
target_link_libraries(${PROJECTNAME} ... icubmod ...)
\endcode 

On the contrary, if you are developing your module externally to the iCub repository, make sure to include the 
following lines:
\code
find_package(ICUB)
...
include_directories(... ${ICUB_INCLUDE_DIRS} ...)
...
target_link_libraries(${PROJECTNAME} ... icubmod ...)
\endcode 

Besides, it is also required to insert the following lines within the code in order to correctly enable the cartesian
client controller:
\code
...
#include <yarp/dev/all.h>
YARP_DECLARE_DEVICES(icubmod)
...

int main()
{
    YARP_REGISTER_DEVICES(icubmod)
    ...
}
\endcode

You are now able to open the Cartesian Interface in the usual way:
\code 
Property option;
option.put("device","cartesiancontrollerclient");
option.put("remote","/icub/cartesianController/right_arm");
option.put("local","/client/right_arm");
 
PolyDriver clientCartCtrl(option);

ICartesianControl *icart=NULL;

if (clientCartCtrl.isValid()) {
   clientCartCtrl.view(icart);
}
\endcode

As you might have noticed, the stem-name of the cartesian server controller (taken as standard) is "/<robot_name>/cartesianController/<part_name>".
Thus, similarly to a usual motor controller, a state port streaming out the cartesian pose of the limb's end-effector is also
available whose name is "/<robot_name>/cartesianController/<part_name>/state:o".
As result you can do:
\code
yarp read /read /icub/cartesianController/right_arm/state:o
\endcode
getting the current pose of the right hand end-effector expressed with seven double \f$ \left[x,y,z,a_x,a_y,a_z,\theta\right] \f$,
where the first three components are the 3d location of the center of the palm, whilst the final four describe the rotation of the
frame attached to the end-effector with respect to the root reference frame in axis/angle notation (see \ref sec_cart_coordsystem).

When you are done with controlling the robot you can explicitly close the device (or just let the destructor do it for you): 
\code
clientCartCtrl.close();
\endcode


\section sec_cart_coordsystem Coordinate System 

Central for a correct usage of the cartesian controller is the knowledge of the coordinate system 
that shall be adopted to describe any desired limb pose.

Positions (in meters) and orientation refer to the root frame attached to the waist as in the
<a href="http://eris.liralab.it/wiki/ICubForwardKinematics">wiki</a> page. \n
The root <b>x-axis</b> points backward, the <b>y-axis</b> points rightward, while the <b>z-axis</b> points upward.
 
To specify a target orientation of the end-effector, you have to provide three components for the rotation
axis (whose norm is 1) and a fourth component for the rotation angle expressed in radians as required by the
axis/angle notation (click <a href="http://en.wikipedia.org/wiki/Axis_angle">here</a>). \n
The axis/angle format compacts the notation and protects from some singularities that might appear when the
Euler angles are adopted, but obviously one can still use his most preferable representation since there exist
transformation formulas from one domain to another (e.g. look at ctrl::dcm2axis or ctrl::axis2dcm documentation). 

<b>Example</b> \n

We want to represent the orientation of the right hand when the arm is in the rest position (the torso and arm joints are zeroed),
for which the x-axis attached to the center of the palm points downward, the y-axis points backward, while the z-axis
points leftward. Hence, to transform the root reference frame in the end-effector frame, one solution is to rotate of pi/2
around the y-axis and then of pi/2 around the resulting x-axis.

In formulas:
\code
Vector oy(4), ox(4);
oy[0]=0.0; oy[1]=1.0; oy[2]=0.0; oy[3]=M_PI/2.0;
ox[0]=1.0; ox[1]=0.0; ox[2]=0.0; ox[3]=M_PI/2.0;

Matrix Ry=ctrl::axis2dcm(oy);   // from axis/angle to rotation matrix notation
Matrix Rx=ctrl::axis2dcm(ox);

Matrix R=Ry*Rx;
Vector o=ctrl::dcm2axis(R);     // from rotation matrix back to the axis/angle notation
\endcode

The outcome is: o = [0.57735 0.57735 -0.57735 2.094395]


\section sec_cart_useinterface Using the Cartesian Interface

The YARP Cartesian Interface is fully documented <a href="http://eris.liralab.it/yarpdoc/classyarp_1_1dev_1_1ICartesianControl.html">here</a>.
Moreover, some examples are also reported hereafter to gain a deeper insight into the cartesian device.

Imagine that we want to get the actual arm pose from within our code. What we have to write is just:
\code
Vector x0,o0;
icart->getPose(x0,o0);
\endcode
The current translational position is returned in <i>x0</i> while <i>o0</i> containes now the current orientation.

Then we want to move the hand a bit farther from the body, let's say just 10 cm away along -x,
keeping the orientation constant:
\code
Vector xd=x0; xd[0]+=-0.1;
Vector od=o0;
icart->goToPose(xd,od);   // send request and forget
\endcode
The <i>goToPose()</i> method does not wait for any acknowledgement that the server has received the request,
but it just lets the client module go straight to the next instruction in the code flow. Therefore, the 
intended use of this method is for streaming input (e.g. while tracking).

If we would like to test whether the new pose has been achieved, it's as follows:
\code
icart->goToPoseSync(xd,od);   // send request and wait for reply

bool done=false;
while (!done) {
   icart->checkMotionDone(&done);
   Time::delay(0.04);   // or any suitable delay
}
\endcode
The same can be achieved also by the following streamlined snippet of code:
\code
icart->goToPoseSync(xd,od);   // send request and wait for reply
icart->waitMotionDone(0.04);  // wait until the motion is done and ping at each 0.04 seconds
\endcode
Here the method <i>goToPoseSync()</i> guarantees that the server receives the request before allowing the code
flow to proceed. If we had used the analogous non-sync method, then the result of first check would have been
unpredictable.

To tune the trajectory execution time for point-to-point movements, you can call the proper function:
\code
icart->setTrajTime(1.5);  // given in seconds
\endcode

To let the controller determine when to declare the movement accomplished, there is a dedicated tolerance acting 
both at the level of position and orientation errors. To set this tolerance you can write the following:
\code
icart->setInTargetTol(0.001);
\endcode
This means that the movement will be intended finished whenever norm(xd-x)<0.001, where x comprises the position
and the orientation of the end-effector.

While moving to a target pose, you can query the controller to find out which will be the final joints configuration
as determined by the solver:
\code
Vector xdhat, odhat, qdhat;
icart->getDesired(xdhat,odhat,qdhat);
\endcode
Of course due to the optimization process \f$ \hat{x}_d \f$ differs from the commanded \f$ x_d \f$ (but the distance in norm complies
with the given tolerance), so as \f$ \hat{o}_d \f$ from \f$ o_d \f$; \f$ \hat{q}_d \f$ is such that \f$ \left[\hat{x}_d,\hat{o}_d\right]=K\left(\hat{q}_d\right) \f$,
where \f$ K\left(\cdot\right) \f$ is the forward kinematic map. \n

By contrast, if you want to solve for a given pose without actually moving there, you can use the ask-based methods as follows:
\code
Vector xd(3);
xd[0]=-0.3;
xd[1]=0.0;
xd[2]=0.1;

Vector xdhat, odhat, qdhat;
icart->askForPosition(xd,xdhat,odhat,qdhat);
\endcode
The ask-based methods such as <i>askForPosition()</i> and <i>askForPose()</i> allow the user to employ the Cartesian Interface
just as a <b>simple layer that wraps around the inverse kinematics solver</b>.

One useful feature is the possibility to enable/disable some degrees of freedom on-line.
For example, the arm comes with 10 possible DOF's (the torso is included) that the user may or may not want to use
at the same time while reaching. \n
In the following snippet of code we ask to enable the torso pitch and yaw joints (which are off by default).
\code
Vector curDof;
icart->getDOF(curDof);
cout<<"["<<curDof.toString()<<"]"<<endl;  // [0 0 0 1 1 1 1 1 1 1] will be printed out

Vector newDof(3);
newDof[0]=1;    // torso pitch: 1 => enable
newDof[1]=2;    // torso roll:  2 => skip
newDof[2]=1;    // torso yaw:   1 => enable
icart->setDOF(newDof,curDof);
cout<<"["<<curDof.toString()<<"]"<<endl;  // [1 0 1 1 1 1 1 1 1 1] will be printed out
\endcode
To sum up, a value of '1' makes the joint a real degree of freedom, a value of '0' switches it off, while the special value
of '2' indicates that the joint status won't be modified (to skip it and proceed forward).

Two things deserve your attention:
- The torso joints order is the one defined by the kinematic description available from the wiki: [pitch,roll,yaw].
  Usually the motor control interfaces stream the torso joints in the reversed order (i.e. [yaw,roll,pitch]) since they assume
  a different kinematic origin (the base of the neck in this case).
- The shoulder joints are considered all together to be a super-joint (in order to take into account the shoulder's cable
  lenght issue) so that you are not allowed to enable/disable one of them differently from the others.

Furthermore, as you can easily figure out, the torso joints are shared by both arms. This tells us to take care of a misuse
of the cartesian device that might arise when one arm is controlled simultaneously to the other one: if this happens, a supervisor
that enables/disables the torso joints according to the needs should be put in place.

When the torso is being controlled by an external module, there exists also the possibility for the cartesian device to counteract
to the induced movements of the uncontrolled joints in order to mantain the final end-effector pose stable. \n
To enable this feature one should call the proper method:
\code
icart->setTrackingMode(true);
\endcode
Usually the cartesian device brings up in <i>non-tracking</i> mode: this means that once the limb has reached the desired pose, the controller
gets disconnected until the next command is yielded and the limb can be moved by other external agents. \n
Conversely, in <i>tracking</i> mode the controller stays connected and tries to compensate for externally induced movements on the uncontrolled
joints (that are seen as disturbs on the actual end-effector position) such as the torso ones (in case they have been previously disabled).

Another advanced feature you can benefit from is the chance to change dynamically the rest position the solver uses to specify a secondary task 
while converging to the solution. \n
Briefly (a detailed description is given in the \ref iKinSlv "solver page"), the cartesian solver implements a nonlinear constrained optimization
that can be described in this way:

\f[
\mathbf{q}=\arg\min_{\mathbf{q}\in R^{n} }\left(\frac{1}{2}\left\|\mathbf{\alpha}_d-\mathit{K_{\alpha}}\left(\mathbf{q}\right)\right\|^2+\mathit{w}\cdot\frac{1}{2}\left(\mathbf{q}_r-\mathbf{q}\right)^{\top}\mathit{W}_r\left(\mathbf{q}_r-\mathbf{q}\right)\right) \quad s.t.\,\left\{\begin{array}{l}\left\|\mathbf{x}_d-\mathit{K_x}\left(\mathbf{q}\right)\right\|^2<\epsilon\\\mathbf{q}_L<\mathbf{q}<\mathbf{q}_U\end{array}\right.
\f]

The term \f$ \frac{1}{2}\left(\mathbf{q}_r-\mathbf{q}\right)^{\top}\mathit{W}_r\left(\mathbf{q}_r-\mathbf{q}\right) \f$ represents
the aforementioned secondary task, where \f$ \mathbf{q}_r \f$ is used to keep the solution as close as possible to a given rest
position in the joint space, weighting component by component with the diagonal matrix \f$ \mathit{W}_r \f$. \n
Thereby, to access the elements of \f$ \mathbf{q}_r \f$ and \f$ \mathit{W}_r \f$ one should call the following methods:
\code
Vector curRestPos;
icart->getRestPos(curRestPos);
cout<<"["<<curRestPos.toString()<<"]"<<endl;  // [0 0 0 0 0 0 0 0 0 0] will be printed out

Vector curRestWeights;
icart->getRestWeights(curRestWeights);
cout<<"["<<curRestWeights.toString()<<"]"<<endl;  // [1 1 1 0 0 0 0 0 0 0] will be printed out
\endcode
Above we've asked for the default configuration of the secondary task which tries to minimize against the rest position (0,0,0) that considers
only the torso joints in the computation. This prevents the robot from leaning out too much if the target position can be reached easily with
just the arm.


\section sec_cart_nonlineardesign Nonlinear Problem Design

In this small section a couple of considerations that motivated the design of the inversion problem are briefly summarized as taken from the
article on the cartesian controller and perhaps more importantly as they came up from discussions with some meticulous users.

One of the main aspects of the nonlinear problem as handled by the cartesian solver is that the reaching for position is a constraint, whereas the
reaching in orientation appears directly in the cost function. As result, the former is treated with higher priority with respect to the latter
to reflect the requirement of achieving the task in a very localized zone of the space. For example, this particular preference is of major
interest for grasping, where having an hand perfectly oriented with respect to the object to be grasped but with a position of even a couple
of centimeters away from the target will cause the task to fail much more frequently than having the hand perfectly located in position and
with even ten degrees out of the alignment.
One behavior emerging from this design that the user may experience effortlessly is the following: when a out-of-range position is commanded
together with a given orientation, it turns that the robot will stretch its body up to its maximum extension (depending on the parts currently
enabled) almost irrespective of the final orientation. As consequence, bear always in mind to enable the torso to enlarge as much as possible
the dexterous space of the iCub.

Conversely, there does not exist a specific reason why an optimizer (or more generally an inverse kinematic algorithm) should be employed to put
the end-effector in a given orientation almost regardless its position in the space, which sounds like with a priority equal to the position:
it would be indeed more conveniently to rely on the pure joint control for these cases.

Nonetheless, \ref iKin provides a bunch of suitable tools to easily interface with Ipopt and develop different kinematic optimization problems:
it will be as straightforward as to inherit C++ classes to describe the cost function to minimize along with the related set of constraints.


\section sec_cart_contextswitch Context Switch

We define here the <i>context</i> as the configuration in which the controller operates: therefore the context includes the actual number of DOF's,
their angular bounds, the resting positions and weights, the trajectory execution time, the current tracking mode and so on.
Obviously the controller performs the same action differently depending on the current context.
A way to easily switch among contexts is to rely on <i>storeContext()</i> and <i>restoreContext()</i> methods as follows:
\code
icart->setDOF(newDof1,curDof1);         // here the user prepares the context
icart->setTrackingMode(true);
...
int context_0;
icart->storeContext(&context_0);        // latch the context
                                        
icart->setDOF(newDof2,curDof2);         // at certain point the user may want the controller to
icart->goToPose(x,o,t);                 // perform some actions with different configuration
...

icart->restoreContext(context_0);       // ... and then retrieve the stored context_0
icart->goToPose(x,o);                   // now the controller performs the same action but within the context_0
\endcode
Unless the user needs the interface just for logging purposes, it's a good rule to store the context at the initialization of his module in order to
then restore it at releasing time to preserve the controller configuration.


\section sec_cart_taskrefvel Task-Space Reference Velocity Command

The user is further provided with the method <i>setTaskVelocities()</i> that allows to command a reference velocity for the end-effector in the
operational space as follows:
\code
Vector xdot(3); // move the end-effector along y-axis at specified velocity
xdot[0]=0.0;    // [m/s]
xdot[1]=0.01;
xdot[2]=0.0;

Vector odot(4); // no rotation is required
odot=0.0;       // [rad/s]

icart->setTaskVelocities(xdot,odot);
\endcode
The solver still plays a meaningful role in this command mode so that the whole set of constraints active in the inverse kinematics problem
are taken into account. Moreover, the task-space reference velocity will be tracked according to the reactivity level of the internal controller
which is basically determined by the current trajectory time; thus, for executing high task-space velocity the trajectory time should be tuned
towards lower values, whereas it should be increased to track low task-space reference velocity.


\section sec_cart_eventscallbacks Events Callbacks

The Cartesian Interface provides also an easy way to register events callbacks. For example, an event might be represented by the onset of
the movement when the controller gets activated or the end of the movement itself. The user can then attach a callback to any event generated
by the interface. \n
It is required to inherit from a specific class <i>CartesianEvent</i> that handles events and override the <i>cartesianEventCallback()</i> method.
\code
class MotionDoneEvent : public CartesianEvent
{
public:
   virtual void cartesianEventCallback()
   {
      cout<<"motion complete"<<endl;
   }
};

MotionDoneEvent event;
icart->registerEvent("motion-done",&event);    // the tag "motion-done" identifies the event to be notified
icart->goToPosition(x);                        // as soon as the motion is complete, the callback will print out the message
\endcode

To know which events are available for notification, the user can do:
\code
Bottle info;
icart->getInfo(info);
cout<<info.find("events").asList()->toString().c_str()<<endl;
\endcode
The special wildcard "*" can be used to assign a callback to any event, regardless of its type.


\section sec_cart_siminterface The Simulator and the Cartesian Interface

To have the Cartesian Interface fully operative together with the robot simulator, please have a look \ref simCartesianControl "here".
You will need to compile the server part of the interface also on the cluster where the simulator is supposed to run.

A working example is available under
\code
tutorials/src/tutorial_cartesian_interface.cpp
\endcode
which makes the left hand follow a circular trajectory located in front of the simulated robot.
*/

